---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
# Load packages
library(readxl)
library(DescTools)
library(irr)
library(ROCR)
library(stringr)
library(xlsx)
library(caret)  # For sensitivity()
library(grDevices)
```


```{r}
# Read data
df <- read_excel("C:\\D\\e Health Lab projects\\Question_Answering\\LLM_Causality\\Causal IRR.xlsx")
df

```

```{r}
binary_cols <- grep("Accurate_Correct_Response", names(df), value = TRUE)
df[binary_cols] <- lapply(df[binary_cols], function(x) ifelse(tolower(x) == "yes", 1, 0))
df[binary_cols]
```

```{r}
# ---- Majority vote with tie-break ----
resolve_majority_vote <- function(row_vals) {
  sum_votes <- sum(row_vals)
  if (sum_votes == 2) {
    agreement_scores <- sapply(1:4, function(i) {
      others <- row_vals[-i]
      mean(row_vals[i] == others)
    })
    rater_to_exclude <- which.min(agreement_scores)
    reduced_votes <- row_vals[-rater_to_exclude]
    return(as.integer(sum(reduced_votes) >= 2))
  } else {
    return(as.integer(sum_votes > 2))
  }
}

# Apply majority vote
df$Majority_Binary <- apply(df[binary_cols], 1, resolve_majority_vote)

# ---- Helper: AUROC calculation ----
safe_auc <- function(y_true, y_pred) {
  if (length(unique(y_pred)) == 1 || length(unique(y_true)) == 1) return(NA)
  pred <- ROCR::prediction(y_pred, y_true)
  perf <- ROCR::performance(pred, "auc")
  return(as.numeric(perf@y.values[[1]]))
}

# ---- Metric function ----
row_metrics <- function(y_true, y_pred) {
  tp <- as.integer(y_true == 1 & y_pred == 1)
  tn <- as.integer(y_true == 0 & y_pred == 0)
  fp <- as.integer(y_true == 0 & y_pred == 1)
  fn <- as.integer(y_true == 1 & y_pred == 0)

  phi_num <- (tp * tn) - (fp * fn)
  phi_den <- sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
  phi <- if (!is.na(phi_den) && phi_den != 0) phi_num / phi_den else NA

  sensitivity <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  specificity <- if ((tn + fp) > 0) tn / (tn + fp) else NA
  precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
  f1 <- if (!is.na(precision) && !is.na(sensitivity) && (precision + sensitivity) > 0) {
    2 * (precision * sensitivity) / (precision + sensitivity)
  } else {
    NA
  }

  return(c(sensitivity = sensitivity, specificity = specificity, precision = precision))
}

# ---- Compute metrics per rater ----
binary_results_all <- list()

for (rater_col in binary_cols) {
  rater_name <- gsub("_Accurate_Correct_Response_Yes_No", "", rater_col)
  df$Binary_ModelOutput <- df[[rater_col]]
  df$Binary_GroundTruth <- df$Majority_Binary

  # GROUPED by LLM + Rung
  by_rung <- df %>%
    group_by(LLM, Rung) %>%
    group_modify(~ {
      y_true <- .x$Binary_GroundTruth
      y_pred <- .x$Binary_ModelOutput

      auc <- safe_auc(y_true, y_pred)
      metrics <- mapply(row_metrics, y_true, y_pred)
      metrics_df <- as.data.frame(t(metrics))
      metrics_df$AUROC <- auc

      summarised <- sapply(metrics_df, function(col) {
        m <- mean(col, na.rm = TRUE)
        s <- sd(col, na.rm = TRUE)
        sprintf("%.2f(%.2f)", m, s)
      })

      data.frame(Metric = names(summarised), Value = summarised)
    }) %>%
    mutate(Rater = rater_name)

  # OVERALL (combine all rungs for each LLM)
  overall <- df %>%
    group_by(LLM) %>%
    group_modify(~ {
      y_true <- .x$Majority_Binary
      y_pred <- .x[[rater_col]]

      auc <- safe_auc(y_true, y_pred)
      metrics <- mapply(row_metrics, y_true, y_pred)
      metrics_df <- as.data.frame(t(metrics))
      metrics_df$AUROC <- auc

      summarised <- sapply(metrics_df, function(col) {
        m <- mean(col, na.rm = TRUE)
        s <- sd(col, na.rm = TRUE)
        sprintf("%.2f(%.2f)", m, s)
      })

      data.frame(Rung = "Overall", Metric = names(summarised), Value = summarised)
    }) %>%
    mutate(Rater = rater_name)

  # Combine rung + overall
  binary_results_all[[rater_name]] <- bind_rows(by_rung, overall)
}

# Combine all raters into one summary table
binary_all_raters_summary <- bind_rows(binary_results_all)

# Preview
print(binary_all_raters_summary)

```
```{r}
library(dplyr)
library(tidyr)

# Split mean(sd) string into numeric mean and sd
binary_summary_final <- binary_all_raters_summary %>%
  tidyr::separate(Value, into = c("mean_val", "sd_val"), sep = "\\(", remove = FALSE) %>%
  mutate(
    mean_val = as.numeric(trimws(mean_val)),
    sd_val = as.numeric(gsub("\\)", "", sd_val))
  ) %>%
  group_by(LLM, Rung, Metric) %>%
  summarise(
    Value = if (all(is.na(mean_val))) {
      "NA"
    } else {
      sprintf("%.2f(%.2f)", mean(mean_val, na.rm = TRUE), sd(mean_val, na.rm = TRUE))
    },
    .groups = "drop"
  ) %>%
  arrange(LLM, factor(Rung, levels = c("Association", "Intervention", "Counterfactual", "Overall")), Metric)

# Final print
print(binary_summary_final)

```
```{r}
# Where AUROC cannot be computed (uniform y_true or y_pred)
df %>%
  group_by(LLM, Rung) %>%
  summarise(
    GroundTruthClasses = length(unique(Majority_Binary)),
    Rater1Classes = length(unique(.[[binary_cols[1]]])),
    .groups = "drop"
  )

```


```{r}
# Save as CSV
write.csv(binary_summary_final, "C:\\D\\e Health Lab projects\\Question_Answering\\LLM_Causality\\Clinican results\\final_llm_rater_metrics.csv", row.names = FALSE)
```


```{r}
# Load required packages
library(dplyr)
library(DescTools)
library(caret)
library(ROCR)

# Identify Likert columns
likert_cols <- grep("Reliable_Reasoning", names(df), value = TRUE)
df[likert_cols] <- lapply(df[likert_cols], as.numeric)

# Compute ground truth: median Likert per row â†’ binary
df$Likert_Median <- apply(df[likert_cols], 1, median)
df$Likert_GroundTruth <- ifelse(df$Likert_Median <= 3, 1, 0)

# Helper for AUROC
safe_auc <- function(y_true, y_pred) {
  if (length(unique(y_pred)) == 1 || length(unique(y_true)) == 1) return(NA)
  pred <- prediction(y_pred, y_true)
  perf <- performance(pred, "auc")
  return(as.numeric(perf@y.values[[1]]))
}

# Row-wise metric function
row_metrics <- function(y_true, y_pred) {
  tp <- as.integer(y_true == 1 & y_pred == 1)
  tn <- as.integer(y_true == 0 & y_pred == 0)
  fp <- as.integer(y_true == 0 & y_pred == 1)
  fn <- as.integer(y_true == 1 & y_pred == 0)

  phi_num <- (tp * tn) - (fp * fn)
  phi_den <- sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
  phi <- if (!is.na(phi_den) && phi_den != 0) phi_num / phi_den else NA

  sensitivity <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  specificity <- if ((tn + fp) > 0) tn / (tn + fp) else NA
  precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
  f1 <- if (!is.na(precision) && !is.na(sensitivity) && (precision + sensitivity) > 0) {
    2 * (precision * sensitivity) / (precision + sensitivity)
  } else {
    NA
  }

  return(c(sensitivity = sensitivity, specificity = specificity, precision = precision))
}

# Initialize final results list
likert_results_all <- list()

# Loop over raters
for (rater_col in likert_cols) {
  rater_name <- gsub("_Reliable_Reasoning_1_high_to_5_poor", "", rater_col)
  df$Likert_ModelOutput <- ifelse(df[[rater_col]] <= 3, 1, 0)

  # Per Rung metrics
  per_rung <- df %>%
    group_by(LLM, Rung) %>%
    group_modify(~ {
      y_true <- .x$Likert_GroundTruth
      y_pred <- .x$Likert_ModelOutput

      auc <- safe_auc(y_true, y_pred)
      metrics <- mapply(row_metrics, y_true, y_pred)
      metrics_df <- as.data.frame(t(metrics))
      metrics_df$AUROC <- auc

      summarised <- sapply(metrics_df, function(col) {
        m <- mean(col, na.rm = TRUE)
        s <- sd(col, na.rm = TRUE)
        sprintf("%.2f(%.2f)", m, s)
      })

      data.frame(Metric = names(summarised), Value = summarised)
    }) %>%
    mutate(Rater = rater_name)

  # Overall metric (all rows per LLM)
  overall <- df %>%
    group_by(LLM) %>%
    group_modify(~ {
      y_true <- .x$Likert_GroundTruth
      y_pred <- ifelse(.x[[rater_col]] <= 3, 1, 0)

      auc <- safe_auc(y_true, y_pred)
      metrics <- mapply(row_metrics, y_true, y_pred)
      metrics_df <- as.data.frame(t(metrics))
      metrics_df$AUROC <- auc

      summarised <- sapply(metrics_df, function(col) {
        m <- mean(col, na.rm = TRUE)
        s <- sd(col, na.rm = TRUE)
        sprintf("%.2f(%.2f)", m, s)
      })

      data.frame(Rung = "Overall", Metric = names(summarised), Value = summarised)
    }) %>%
    mutate(Rater = rater_name)

  # Combine both
  likert_results_all[[rater_name]] <- bind_rows(per_rung, overall)
}

# Combine all raters' results
likert_all_raters_summary <- bind_rows(likert_results_all)

# Average across raters: final summary
likert_summary_final <- likert_all_raters_summary %>%
  tidyr::separate(Value, into = c("mean_val", "sd_val"), sep = "\\(", remove = FALSE) %>%
  mutate(
    mean_val = as.numeric(trimws(mean_val)),
    sd_val = as.numeric(gsub("\\)", "", sd_val))
  ) %>%
  group_by(LLM, Rung, Metric) %>%
  summarise(
    Value = if (all(is.na(mean_val))) {
      "NA"
    } else {
      sprintf("%.2f(%.2f)", mean(mean_val, na.rm = TRUE), sd(mean_val, na.rm = TRUE))
    },
    .groups = "drop"
  ) %>%
  arrange(LLM, factor(Rung, levels = c("Association", "Intervention", "Counterfactual", "Overall")), Metric)

# Final output
print(likert_summary_final)


```


```{r}
# Save as CSV
write.csv(likert_summary_final, "C:\\D\\e Health Lab projects\\Question_Answering\\LLM_Causality\\Clinican results\\likert_summary_final.csv", row.names = FALSE)
```

